{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the News Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Codecademy practice project from the <a href='https://www.codecademy.com/learn/paths/data-science'>Data Science Path</a> Natural Languages Processing (NLP) Course, Term Frequency-Inverse Document Frequency Section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newspapers and their online formats supply the public with the information we need to understand the events occurring in the world around us. From politics to sports, the news keeps us informed, in the loop, and ready to make decisions about how to act in a rapidly changing world.<br>\n",
    "<br>\n",
    "Given the vast amount of news articles in circulation, identifying and organizing articles by topic is a useful activity. This can help you sift through the enormous amount of information out there so you can find the news relevant to your interests, or even allow you to build a news recommendation engine!<br>\n",
    "<br>\n",
    "The <a href=\"https://www.thenews.com.pk/\">News International</a> is the largest English language newspaper in Pakistan, covering local and international news across a variety of sectors. A selection of articles from a <a href=\"https://www.kaggle.com/asad1m9a9h6mood/news-articles\">Kaggle Dataset of The News International articles</a> is provided in the workspace.<br>\n",
    "<br>\n",
    "In this project I used term frequency-inverse document frequency (tf-idf) to analyze each article’s content and uncover the terms that best describe each article, providing quick insight into each article’s topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Goal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze news documents using TF-IDF MLP supervised machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be familiar with:\n",
    "- Python3\n",
    "- NLP (Natural Languages Processing)\n",
    "<br><br>\n",
    "- The Python Libraries:\n",
    "    - Pandas\n",
    "    - NLKT\n",
    "    - Sklearn\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://www.alex-ricciardi.com/post/read-the-news-analysis'>My Project Blog Presentation<a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='color : MediumBlue'>Text Pre-processing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before a text can be processed by a NLP model, the text data needs to be pre-processed.\n",
    "Text data pre-processing is the process of  cleaning and prepping  the text data to be processed by NLP models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Cleaning and prepping tasks:\n",
    "- Noise removal is a text pre-processing step concerned with removing unnecessary formatting from our text.\n",
    "- Tokenization is a text pre-processing step devoted to breaking up text into smaller units (usually words or discrete terms).\n",
    "- Normalization is the name we give most other text pre-processing tasks, including stemming, lemmatization, upper and lowercasing, and stopwords removal.\n",
    "    - Stemming is the normalization pre-processing task focused on removing word affixes.\n",
    "    - Lemmatization is the normalization pre-processing task that more carefully brings words down to their root forms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex\n",
    "import re\n",
    "# Natural Language Toolkit - https://www.nltk.org/ -\n",
    "import nltk\n",
    "# Lexical database of English\n",
    "from nltk.corpus import wordnet\n",
    "# Stop words \n",
    "from nltk.corpus import stopwords\n",
    "# Tokenization into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "# lemmatization class\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Counter Dictionary class - https://docs.python.org/3/library/collections.html#collections.Counter -\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:<br>\n",
    "NLTK comes with data packages (corpora, toy grammars, trained models, ect).<br>\n",
    "To install the data, first <a href='http://www.nltk.org/install.html'>install NLTK</a>, then install <a href='http://www.nltk.org/data.html'> install NLTK_data.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Initialization:</b>\n",
    "- of stop words from the English language\n",
    "- of the text normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "normalizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style='color : DarkMagenta'>Part-of-Speech Tagging</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://nlp.stanford.edu/software/tagger.shtml#:~:text=A%20Part%2DOf%2DSpeech%20Tagger,like%20'noun%2Dplural'.\">Part-of-Speech Tagging</a> is the process of reading text in some language and assigns parts of speech to each word (and other token), such as noun, verb, adjective, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the performance of <a href=\"https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\">lemmatization</a> (bring a word to his root), each word in the processed text is assigned parts of speech tag, such as noun, verb, adjective, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part-of-Speech tagging function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```get_part_of_speech()``` function:\n",
    "- Takes the arguments:\n",
    "    - ```word```, string data type.<br>\n",
    "<br>\n",
    "- Matches ```word``` with synonyms\n",
    "- Tags ```word``` and count tags.<br> \n",
    "<br>\n",
    "- Returns The most common tag, the tag with the highest count, ex: n for Noun, string data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part_of_speech(word):\n",
    "    # Synonyms matching\n",
    "    probable_part_of_speech = wordnet.synsets(word)\n",
    "    # Initializing Counter class object\n",
    "    pos_counts = Counter()\n",
    "    # Taging and counting tags\n",
    "    pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  ) # Noun\n",
    "    pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  ) # Verb\n",
    "    pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  ) # Adjectif\n",
    "    pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  ) # Adverb\n",
    "    # The most common tag, the tag with the highest count, ex: n for Noun \n",
    "    most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "    \n",
    "    return most_likely_part_of_speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style='color : DarkMagenta'> Pre-processing text main function:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```preprocess_text()``` function:\n",
    "- Takes the arguments:\n",
    "    - ```text```, string data type.<br>\n",
    "<br>\n",
    "- Cleans ```text``` \n",
    "- Tokenizes ```text```\n",
    "- Normalizes ```text```<br> \n",
    "<br>\n",
    "- Returns the ```normalized``` text, pre-processed text, string data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    # Removes stopwords\n",
    "    tokenized_no_stopwords = [word for word in tokenized if word not in stop_words]\n",
    "    # lemmatizes \n",
    "    normalized = \" \".join([normalizer.lemmatize(token, get_part_of_speech(token)) \\\n",
    "                                                           for token in tokenized_no_stopwords if not re.match(r'\\d+',token)])\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='color : MediumBlue'>Articles Analysis</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis' goal is to uncover the terms that best describe each article and predict each article's topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Convert a collection of text documents to a matrix of token counts, Bag-of-Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Convert a collection of raw documents to a matrix of tf-idf scores, and BoW matrix to tf-idf scores\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "# Data \n",
    "from articles import articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SYDNEY: Cricket fever has gripped Australia with the World Cup just days away. Fans from around the world have thronged to the country and hotels are capitalising. Prices of rooms have almost doubled to 300 dollars and hotels are experiencing full bookings. Experts estimate that during the mega event Australia will generate 1.5 million US dollars just from hotel bookings. If the cost of internal air travel, taxis and tickets is taken into consideration, Australia stands to generate two million US dollars during the World Cup.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Articles sample\n",
    "articles[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process Articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sydney cricket fever grip australia world cup day away fan around world throng country hotel capitalise price room almost double dollar hotel experience full book expert estimate mega event australia generate million u dollar hotel book cost internal air travel taxi ticket take consideration australia stand generate two million u dollar world cup\n"
     ]
    }
   ],
   "source": [
    "processed_articles = [preprocess_text(article) for article in articles]\n",
    "# Preprocess Articles sample\n",
    "print(processed_articles[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style='color : DarkMagenta'>TF-IDF:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Term frequency-inverse document frequency, known as tf-idf, is a numerical statistic used to indicate how important a word is to each document in a collection of documents\n",
    "- tf-idf consists of two components, term frequency and inverse document frequency\n",
    "term frequency is how often a word appears in a document. This is the same as bag-of-words’ word count\n",
    "- inverse document frequency is a measure of how often a word appears across all documents of a corpus\n",
    "- tf-idf is calculated as the term frequency multiplied by the inverse document frequency, the calculated tf-idf is also referred as a Tf-idf score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='color : mediumblue'>Tf-idf Scores:</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf scores are calculated on a term-document basis. That means there is a tf-idf score for each word, for each document. The tf-idf score for some term t in a document d in some corpus is calculated as follows:<br>\n",
    "<br>\n",
    "<i><font size=3>tfidf(t,d) = tf(t,d) * idf(t,corpus)</font></i>\n",
    "\n",
    "- tf(t,d) is the term frequency of term t in document d\n",
    "- idf(t,corpus) is the inverse document frequency of a term t across corpus\n",
    "<br><br>\n",
    "Note: the term frequency element is also referred as the Bag-of-words model results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python, the tf-idf values for each term-document pair in a corpus can easily be calculated using scikit-learn’s ```TfidfVectorizer```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes variable to class, empty score \n",
    "# The norm=None keyword argument prevents scikit-learn from modifying the multiplication of term frequency and inverse\n",
    "corpus_vectorizer = TfidfVectorizer(norm=None)\n",
    "#  Fits/transforms training data and returns a score matrix \n",
    "corpus_tfidf_scores = corpus_vectorizer.fit_transform(processed_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style='color : DarkGreen'>Bag-of-Words to tf-idf:</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like showed in the above cell, ```TfidfVectorizer``` can directly calculat the tf-idf scores for a set of terms across a corpus, but for the seek of this exercise, I created a Bag-of-Words model, and converted the bag-of-words model into tf-idf scores using scikit-learn’s ```TfidfTransformer```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-Words:\n",
    "- Bag-of-Words (BoW), also referred to as the unigram model, is a statistical language model based on word count.\n",
    "- BoW can be implemented as a Python dictionary with each key set to a word and each value set to the number of times that word appears in a text.\n",
    "- For BoW, training data is the text that is used to build a BoW model.\n",
    "- BoW test data is the new text that is converted to a BoW vector using a trained features dictionary.\n",
    "- A feature vector is a numeric depiction of an item’s salient features.\n",
    "- Feature extraction (or vectorization) is the process of turning text into a BoW vector.\n",
    "- A features dictionary is a mapping of each unique word in the training data to a unique index. This is used to build out BoW vectors.\n",
    "- BoW has less data sparsity than other statistical models. It also suffers less from overfitting.\n",
    "- BoW has higher perplexity than other models, making it less ideal for language prediction.\n",
    "- One solution to overfitting is language smoothing, in which a bit of probability is taken from known words and allotted to unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes variable to class, empty BoW vector\n",
    "vectorizer = CountVectorizer()\n",
    "# Fits/transforms training data and returns a BoW matrix, words count \n",
    "bow_matrix = vectorizer.fit_transform(processed_articles)\n",
    "\n",
    "#  ----------------- Stores BoW vectors results in a DataFrame\n",
    "\n",
    "# Gets vocabulary of terms\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "# Creates an articles index\n",
    "articles_index = [f\"Article {i+1}\" for i in range(len(articles))]\n",
    "# Creates pandas DataFrame with the features names\n",
    "# The .T stand for transpose() and todense() returns a dense matrix representation\n",
    "df_bag_of_words = pd.DataFrame(bow_matrix.T.todense(), index=feature_names, columns=articles_index)\n",
    "df_bag_of_words.to_csv('data/articles_bow.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article 1</th>\n",
       "      <th>Article 2</th>\n",
       "      <th>Article 3</th>\n",
       "      <th>Article 4</th>\n",
       "      <th>Article 5</th>\n",
       "      <th>Article 6</th>\n",
       "      <th>Article 7</th>\n",
       "      <th>Article 8</th>\n",
       "      <th>Article 9</th>\n",
       "      <th>Article 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abbasi</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abide</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accord</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>add</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agency</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yi</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yuan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>313 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Article 1  Article 2  Article 3  Article 4  Article 5  Article 6  \\\n",
       "abbasi          0          0          0          1          0          0   \n",
       "abide           1          0          0          0          0          0   \n",
       "accord          0          0          1          0          0          0   \n",
       "add             1          0          0          0          0          0   \n",
       "agency          0          0          0          0          0          0   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "world           0          0          0          0          0          3   \n",
       "would           0          0          0          1          0          0   \n",
       "year            0          1          0          0          0          0   \n",
       "yi              0          0          0          0          0          0   \n",
       "yuan            0          0          0          0          0          0   \n",
       "\n",
       "        Article 7  Article 8  Article 9  Article 10  \n",
       "abbasi          0          0          0           0  \n",
       "abide           0          0          0           0  \n",
       "accord          0          0          0           0  \n",
       "add             0          0          1           0  \n",
       "agency          1          0          0           0  \n",
       "...           ...        ...        ...         ...  \n",
       "world           0          0          0           0  \n",
       "would           0          0          1           0  \n",
       "year            0          0          0           0  \n",
       "yi              0          0          0           2  \n",
       "yuan            0          0          0           2  \n",
       "\n",
       "[313 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts the bag-of-words model to tf-idf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes variable to class\n",
    "transformer = TfidfTransformer(norm=None)\n",
    "# Transfoms and retunrs scores\n",
    "tfidf_scores = transformer.fit_transform(bow_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves score results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ----------------- Stores scores results from TfidfVectorizer in a DataFrame\n",
    "df_corpus_tfidf_scores = pd.DataFrame(corpus_tfidf_scores.T.todense(), index=feature_names, columns=articles_index)\n",
    "df_corpus_tfidf_scores.to_csv('data/articles_corpus_tfidf_scores.csv')\n",
    "#  ----------------- Stores scores results from TfidfTransformer in a DataFrame\n",
    "df_tfidf_scores = pd.DataFrame(tfidf_scores.T.todense(), index=feature_names, columns=articles_index)\n",
    "df_tfidf_scores.to_csv('data/articles_tfidf_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article 1</th>\n",
       "      <th>Article 2</th>\n",
       "      <th>Article 3</th>\n",
       "      <th>Article 4</th>\n",
       "      <th>Article 5</th>\n",
       "      <th>Article 6</th>\n",
       "      <th>Article 7</th>\n",
       "      <th>Article 8</th>\n",
       "      <th>Article 9</th>\n",
       "      <th>Article 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abbasi</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.704748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abide</th>\n",
       "      <td>2.704748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accord</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.704748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>add</th>\n",
       "      <td>2.299283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.299283</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agency</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.704748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.114244</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.299283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.299283</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.704748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yi</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.409496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yuan</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.409496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>313 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Article 1  Article 2  Article 3  Article 4  Article 5  Article 6  \\\n",
       "abbasi   0.000000   0.000000   0.000000   2.704748        0.0   0.000000   \n",
       "abide    2.704748   0.000000   0.000000   0.000000        0.0   0.000000   \n",
       "accord   0.000000   0.000000   2.704748   0.000000        0.0   0.000000   \n",
       "add      2.299283   0.000000   0.000000   0.000000        0.0   0.000000   \n",
       "agency   0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "world    0.000000   0.000000   0.000000   0.000000        0.0   8.114244   \n",
       "would    0.000000   0.000000   0.000000   2.299283        0.0   0.000000   \n",
       "year     0.000000   2.704748   0.000000   0.000000        0.0   0.000000   \n",
       "yi       0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
       "yuan     0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
       "\n",
       "        Article 7  Article 8  Article 9  Article 10  \n",
       "abbasi   0.000000        0.0   0.000000    0.000000  \n",
       "abide    0.000000        0.0   0.000000    0.000000  \n",
       "accord   0.000000        0.0   0.000000    0.000000  \n",
       "add      0.000000        0.0   2.299283    0.000000  \n",
       "agency   2.704748        0.0   0.000000    0.000000  \n",
       "...           ...        ...        ...         ...  \n",
       "world    0.000000        0.0   0.000000    0.000000  \n",
       "would    0.000000        0.0   2.299283    0.000000  \n",
       "year     0.000000        0.0   0.000000    0.000000  \n",
       "yi       0.000000        0.0   0.000000    5.409496  \n",
       "yuan     0.000000        0.0   0.000000    5.409496  \n",
       "\n",
       "[313 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style='color : DarkGreen'>Scores results comparison:</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirms that the tf-idf scores given by ```TfidfTransformer``` and ```TfidfVectorizer``` are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the tf-idf scores the same? : yes\n"
     ]
    }
   ],
   "source": [
    "if np.allclose(corpus_tfidf_scores.todense(), tfidf_scores.todense()):\n",
    "    print('Are the tf-idf scores the same? : yes')\n",
    "else:\n",
    "    print('Are the tf-idf scores the same?: No, something is wrong :(')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='color : MediumBlue'>Results Analysis</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze the results, I use the process of labeling each ```article```'s highest-scoring tf-idf term to determined each ```article```'s ```topic```.<br>\n",
    "While the process of labeling the highest-scoring tf-idf term is a more naive approach than others, it is a quick and easy way of getting insight into the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Article 1</th>\n",
       "      <td>fare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article 2</th>\n",
       "      <td>hong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article 3</th>\n",
       "      <td>sugar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article 4</th>\n",
       "      <td>petrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article 5</th>\n",
       "      <td>engine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article 6</th>\n",
       "      <td>australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article 7</th>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article 8</th>\n",
       "      <td>railway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article 9</th>\n",
       "      <td>cabinet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article 10</th>\n",
       "      <td>china</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Topic\n",
       "Article 1        fare\n",
       "Article 2        hong\n",
       "Article 3       sugar\n",
       "Article 4      petrol\n",
       "Article 5      engine\n",
       "Article 6   australia\n",
       "Article 7         car\n",
       "Article 8     railway\n",
       "Article 9     cabinet\n",
       "Article 10      china"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Articles highest-scoring tf-idf terms aka topics \n",
    "# Creates a list of the indexes' highest value (pandas.Series.idxmax()) \n",
    "topics = [df_tfidf_scores[f'Article {i+1}'].idxmax() for i in range(len(articles_index))] \n",
    "# Creates a Dataframe   \n",
    "df_articles_topics = pd.DataFrame({'Topic' : topics}, index = articles_index)\n",
    "df_articles_topics.to_csv('data/articles_topics.csv')\n",
    "df_articles_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using NLP supervised machine learning models we can gain insight into news articles topics without having  the need to read them.<br>\n",
    "For example, we can determine, with good certainty, that the article-6's topic is linked to Australia."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
